{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c6c784-b13c-4c73-a2c8-0acab07e58cd",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates the following processes:\n",
    "1. Downloading loading displacement time series from the EOST and ESMGFZ services.\n",
    "2. Applying these displacement series as non-tidal loading (NTL) corrections to GNSS position time series.\n",
    "3. Saving the corrected data in a format compatible with the Hector software for further analysis.\n",
    "\n",
    "The workflow includes:\n",
    "- Retrieving loading displacement data.\n",
    "- Preprocessing and aligning loading data with GNSS station records.\n",
    "- Applying corrections and generating corrected time series for subsequent analysis.\n",
    "\n",
    "**Requirements**: Python ≥ 3.8 with packages `pandas`, `numpy`, `matplotlib`  \n",
    "**Note**: Ensure that file paths to the GNSS and loading data match your directory structure.\n",
    "\n",
    "**Outputs**:\n",
    "- Corrected GNSS time series in `.mom` format, ready for Hector processing.\n",
    "- Each `.mom` file contains a first-line comment indicating the sampling period, followed by space-separated displacement values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c66f23a3-f15b-4163-a267-30adcd998e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timezone, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e4efc-5c0a-46c1-8b52-4c52f95a0910",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "This sectiont includes a set of helper functions to support the processing of station data and the application of NTL corrections.\n",
    "\n",
    "- **`datetime_to_mjd(date_obj)` / `mjd_to_datetime(mjd)`**  \n",
    "  Convert between Python `datetime` objects and Modified Julian Dates (MJD). These functions allow consistent time handling when reading, merging, or processing time series data from different sources.\n",
    "\n",
    "- **`xyz_to_enu(dx, dy, dz, lat_deg, lon_deg)`**  \n",
    "  Transform ECEF (XYZ) coordinate differences into local East-North-Up (ENU) displacements. Used to convert global Cartesian GNSS positions into local station-centric displacements.\n",
    "\n",
    "- **`process_station_displacements(station_name, station_data)`**  \n",
    "  Process GNSS station data by computing relative ECEF displacements from a reference epoch and converting them to ENU coordinates in millimeters. This function prepares the time series for analysis and comparison with non-tidal loading corrections.\n",
    "\n",
    "**Note**: Not all functions used in this notebook are defined here; some are introduced later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e421cccc-7b04-4abf-b64f-3d476193eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_to_mjd(date_obj):\n",
    "    \"\"\"\n",
    "    Convert a datetime or ISO-format string to Modified Julian Date (MJD).\n",
    "\n",
    "    Parameters:\n",
    "    - date_obj: datetime object or string in \"YYYY-MM-DD\" or \"YYYY-MM-DDTHH:MM:SS\" format.\n",
    "\n",
    "    Returns:\n",
    "    - mjd: float representing Modified Julian Date\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(date_obj, str):\n",
    "        try:\n",
    "            # Try parsing as ISO format (with time)\n",
    "            date_obj = datetime.strptime(date_obj, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        except ValueError:\n",
    "            try:\n",
    "                # Fall back to date-only format\n",
    "                date_obj = datetime.strptime(date_obj, \"%Y-%m-%d\")\n",
    "            except ValueError as e:\n",
    "                raise ValueError(f\"Unsupported date format. Expected '%Y-%m-%dT%H:%M:%S' or '%Y-%m-%d'. Got: {date_obj}\") from e\n",
    "    \n",
    "    unix_epoch = datetime(1970, 1, 1)\n",
    "    jd = 2440587.5 + (date_obj - unix_epoch).total_seconds() / 86400\n",
    "    mjd = jd - 2400000.5\n",
    "    return mjd\n",
    "    \n",
    "\n",
    "def mjd_to_datetime(mjd):\n",
    "    \"\"\"\n",
    "    Convert Modified Julian Date (MJD) to a Python datetime object in UTC.\n",
    "\n",
    "    Parameters: \n",
    "    - mjd: float representing Modified Julian Date\n",
    "\n",
    "    Returns: \n",
    "    - date_obj: datetime object in UTC\n",
    "    \"\"\"\n",
    "    \n",
    "    mjd = mjd + 2400000.5  # Convert MJD to Julian Date\n",
    "    unix_epoch = datetime(1970, 1, 1, tzinfo=timezone.utc)\n",
    "    date_obj = unix_epoch + timedelta(days=(mjd - 2440587.5))  # Convert JD to UTC datetime\n",
    "    return date_obj  # Return datetime object instead of string\n",
    "    \n",
    "\n",
    "def xyz_to_enu(dx, dy, dz, lat_deg, lon_deg):\n",
    "    \"\"\"\n",
    "    Convert ECEF coordinate differences to ENU displacements.\n",
    "\n",
    "    Parameters:\n",
    "    - dx, dy, dz: coordinate differences in meters (pandas.Series or arrays)\n",
    "    - lat_deg, lon_deg: station latitude and longitude in degrees\n",
    "\n",
    "    Returns: \n",
    "    - numpy array of shape (n, 3) with ENU displacements in meters\n",
    "    \"\"\"\n",
    "    \n",
    "    lat = np.deg2rad(lat_deg)\n",
    "    lon = np.deg2rad(lon_deg)\n",
    "\n",
    "    # Rotation matrix for ECEF to ENU conversion\n",
    "    R = np.array([\n",
    "        [-np.sin(lon),              np.cos(lon),              0],\n",
    "        [-np.sin(lat)*np.cos(lon), -np.sin(lat)*np.sin(lon), np.cos(lat)],\n",
    "        [ np.cos(lat)*np.cos(lon),  np.cos(lat)*np.sin(lon), np.sin(lat)]\n",
    "    ])\n",
    "\n",
    "    # Convert Series to numpy arrays and stack them\n",
    "    dxyz = np.stack([dx.to_numpy(), dy.to_numpy(), dz.to_numpy()], axis=0)\n",
    "    \n",
    "    # Calculate ENU coordinates\n",
    "    enu = R @ dxyz\n",
    "\n",
    "    return enu.T\n",
    "\n",
    "\n",
    "def process_station_displacements(station_name, station_data):\n",
    "    \"\"\"\n",
    "    Process station displacement data by converting ECEF coordinates to ENU coordinates and computing\n",
    "    relative differences from the reference epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - station_name: string key for the station (used to look up latitude/longitude)\n",
    "    - station_data: pandas.DataFrame with columns ['date', 'x', 'y', 'z'] (ECEF coordinates)\n",
    "    \n",
    "    Returns:\n",
    "    - station_data: DataFrame with additional columns ['east', 'north', 'up'] in millimeters.\n",
    "    \"\"\"\n",
    "    # Stations coordinates (lat, lon)\n",
    "    stations = {\n",
    "    'ABOA': {'lat': -73.043771, 'lon': -13.407135},\n",
    "    'SYOG': {'lat': -69.006958, 'lon': 39.583745},\n",
    "    'VESL': {'lat': -71.673797, 'lon': -2.841783}\n",
    "    }\n",
    "\n",
    "    # Extract latitude and longitude from stations dictionary\n",
    "    lat_deg = stations[station_name]['lat']\n",
    "    lon_deg = stations[station_name]['lon']\n",
    "\n",
    "    # Convert date to datetime if necessary and reset index\n",
    "    station_data['date'] = pd.to_datetime(station_data['date'])\n",
    "    station_data = station_data.reset_index(drop=True)\n",
    "\n",
    "    # Calculate reference coordinates (performance implications as you rely on the first entry)\n",
    "    ref_X, ref_Y, ref_Z = station_data.loc[0, ['x', 'y', 'z']]\n",
    "    station_data['dX'] = station_data['x'] - ref_X\n",
    "    station_data['dY'] = station_data['y'] - ref_Y\n",
    "    station_data['dZ'] = station_data['z'] - ref_Z\n",
    "\n",
    "    # Calculate ENU displacements and convert to millimeters (assuming inputs are meters)\n",
    "    enu_displacements = xyz_to_enu(station_data['dX'], station_data['dY'], station_data['dZ'], lat_deg, lon_deg)\n",
    "    station_data[['east', 'north', 'up']] = enu_displacements * 1000  # Convert to mm\n",
    "\n",
    "    return station_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eec041-5c55-49f6-8613-113d595a4016",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Downloading NTL data\n",
    "This section downloads non-tidal loading (NTL) displacement time series from two main services:\n",
    "\n",
    "1. **[EOST (University of Strasbourg)](http://loading.u-strasbg.fr/surface_gravity.php)**\n",
    "   - Models: ERA5_IB, ERA5_TUGO, MERRA2, and ECCO2 atmospheric/ocean loading models.  \n",
    "   - Data is retrieved directly from fixed station URLs for each GNSS site (ABOA, SYOG, VESL). Other stations are also possible, more information from [here](http://loading.u-strasbg.fr/displ_all.php).\n",
    "   - Output: One `.txt` file per station–model combination, saved in the `EOST/` directory.\n",
    "\n",
    "2. **[ESMGFZ (GFZ Potsdam)](https://rz-vm480.gfz.de/)**  \n",
    "   - Models:  \n",
    "     - **NTAL** – Atmospheric loading (ECMWF operational)\n",
    "     - **NTOL** – Ocean loading (MPIOM)\n",
    "     - **HYDL** – Hydrological loading (LSDM)\n",
    "   - Data is requested using latitude/longitude coordinates for each station and dataset-specific entry IDs.  \n",
    "   - Time span is divided into three periods (1995–1999, 2000–2009, 2010–present) to match GFZ’s repository structure.  \n",
    "   - Each period is downloaded as a `.csv` file, then merged into a single combined CSV per station and model type.  \n",
    "   - Output: Merged CSV files stored in the `ESMGFZ/` directory for later processing.\n",
    "\n",
    "**Requirements**: Internet connection and write access to the target directories (`EOST/` and `ESMGFZ/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae280f-a29a-4b8b-9b02-426966ef87dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EOST data addresses\n",
    "syog_era5ib_link = 'http://loading.u-strasbg.fr/ITRF/CF//ERA5_IB/SYQB_66006S005_NEU.era5'\n",
    "vesl_era5ib_link = 'http://loading.u-strasbg.fr/ITRF/CF//ERA5_IB/VESL_66009M001_NEU.era5'\n",
    "aboa_era5ib_link = 'http://loading.u-strasbg.fr/ITRF/CF//ERA5_IB/ABOA_XXXXXXXXX_NEU.era5'\n",
    "\n",
    "syog_era5tugo_link = 'http://loading.u-strasbg.fr/ITRF/CF//ERA5_TUGO/SYQB_66006S005_NEU.era5'\n",
    "vesl_era5tugo_link = 'http://loading.u-strasbg.fr/ITRF/CF//ERA5_TUGO/VESL_66009M001_NEU.era5'\n",
    "aboa_era5tugo_link = 'http://loading.u-strasbg.fr/ITRF/CF//ERA5_TUGO/ABOA_XXXXXXXXX_NEU.era5'\n",
    "\n",
    "syog_ecco2_link = 'http://loading.u-strasbg.fr/ITRF/CF//ECCO2/SYQB_66006S005_NEU.ecco2'\n",
    "vesl_ecco2_link = 'http://loading.u-strasbg.fr/ITRF/CF//ECCO2/VESL_66009M001_NEU.ecco2'\n",
    "aboa_ecco2_link = 'http://loading.u-strasbg.fr/ITRF/CF//ECCO2/ABOA_XXXXXXXXX_NEU.ecco2'\n",
    "\n",
    "syog_merra2_link = 'http://loading.u-strasbg.fr/ITRF/CF//MERRA2_atm/SYQB_66006S005_NEU_ib.merra2'\n",
    "vesl_merra2_link = 'http://loading.u-strasbg.fr/ITRF/CF//MERRA2_atm/VESL_66009M001_NEU_ib.merra2'\n",
    "aboa_merra2_link = 'http://loading.u-strasbg.fr/ITRF/CF//MERRA2_atm/ABOA_XXXXXXXXX_NEU_ib.merra2'\n",
    "\n",
    "# Request data\n",
    "urllib.request.urlretrieve(aboa_era5ib_link, 'EOST/ABOA_era5ib.txt')\n",
    "urllib.request.urlretrieve(aboa_era5tugo_link, 'EOST/ABOA_era5tugo.txt')\n",
    "urllib.request.urlretrieve(aboa_ecco2_link, 'EOST/ABOA_ecco2.txt')\n",
    "urllib.request.urlretrieve(aboa_merra2_link, 'EOST/ABOA_merra2.txt')\n",
    "\n",
    "urllib.request.urlretrieve(syog_era5ib_link, 'EOST/SYOG_era5ib.txt')\n",
    "urllib.request.urlretrieve(syog_era5tugo_link, 'EOST/SYOG_era5tugo.txt')\n",
    "urllib.request.urlretrieve(syog_ecco2_link, 'EOST/SYOG_ecco2.txt')\n",
    "urllib.request.urlretrieve(syog_merra2_link, 'EOST/SYOG_merra2.txt')\n",
    "\n",
    "urllib.request.urlretrieve(vesl_era5ib_link, 'EOST/VESL_era5ib.txt')\n",
    "urllib.request.urlretrieve(vesl_era5tugo_link, 'EOST/VESL_era5tugo.txt')\n",
    "urllib.request.urlretrieve(vesl_ecco2_link, 'EOST/VESL_ecco2.txt')\n",
    "urllib.request.urlretrieve(vesl_merra2_link, 'EOST/VESL_merra2.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f57815-9709-4a85-adf6-f9532e909fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define station coordinates for downloading ESMGFZ data\n",
    "stations = {\n",
    "    'ABOA': {'lat': -73.043771, 'lon': -13.407135},\n",
    "    'SYOG': {'lat': -69.006958, 'lon': 39.583745},\n",
    "    'VESL': {'lat': -71.673797, 'lon': -2.841783}\n",
    "}\n",
    "\n",
    "# Define entry IDs for each dataset and time period\n",
    "entry_ids = {\n",
    "    \"NTAL\": {\n",
    "        \"1995-1999\": \"ecdac7ab-20c3-4622-8936-847899650473\",\n",
    "        \"2000-2009\": \"045c6abc-dd44-47e5-b8bf-67a1645ced28\",\n",
    "        \"2010-now\": \"394c47ee-32e7-462e-83d0-0142ea060e5b\",\n",
    "    },\n",
    "    \"NTOL\": {\n",
    "        \"1995-1999\": \"a283b887-a162-42d3-a7d0-a797d7dca617\",\n",
    "        \"2000-2009\": \"c7110c00-dfd4-433a-b83d-fd839685b394\",\n",
    "        \"2010-now\": \"91ee389e-30dc-4781-af97-9e5f0908edec\",\n",
    "    },\n",
    "    \"HYDL\": {\n",
    "        \"1995-1999\":\"4050364c-e021-461b-9437-1222a4d32368\",\n",
    "        \"2000-2009\": \"cbdf81e0-cfb4-49de-a141-e17bb103a742\",\n",
    "        \"2010-now\": \"bb87d9a7-ceb3-419d-a296-f739076dd1b2\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define time periods (10-year intervals)\n",
    "time_periods = {\n",
    "    \"1995-1999\": (\"1995-01-01\", \"1999-12-31\"),\n",
    "    \"2000-2009\": (\"2000-01-01\", \"2009-12-31\"),\n",
    "    \"2010-now\": (\"2010-01-01\", \"2024-12-31\"),\n",
    "}\n",
    "\n",
    "# Define variables\n",
    "variables = \"&variable=duEW&variable=duNS&variable=duV\"\n",
    "\n",
    "# Define base URL format\n",
    "base_url = \"http://rz-vm115.gfz-potsdam.de:8080/repository/entry/show/{}_point\"\n",
    "\n",
    "# Create output directory if it doesn’t exist\n",
    "output_dir = \"ESMGFZ\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through stations, datasets, and time periods\n",
    "for station, coords in stations.items():\n",
    "    lat, lon = coords['lat'], coords['lon']\n",
    "    \n",
    "    for dataset, periods in entry_ids.items():\n",
    "        for period, dates in time_periods.items():\n",
    "            # Determine the correct entry ID\n",
    "            entry_id = periods[period]\n",
    "\n",
    "            # Construct the request URL\n",
    "            url = (\n",
    "                f\"{base_url.format(period)}\"\n",
    "                f\"?submit=Get%20Data&output=data.gridaspoint\"\n",
    "                f\"&entryid={entry_id}\"\n",
    "                f\"&location.latitude={lat}&location.longitude={lon}\"\n",
    "                f\"&calendar=proleptic_gregorian\"\n",
    "                f\"&fromdate={dates[0]}%2000%3A00%3A00%20UTC\"\n",
    "                f\"&todate={dates[1]}%2021%3A00%3A00%20UTC\"\n",
    "                f\"&format=csv{variables}\"\n",
    "            )\n",
    "\n",
    "            # Define filename\n",
    "            file_name = f\"{output_dir}/{station}/{station}_{dataset}_{period}.csv\" \n",
    "\n",
    "            # Download the file\n",
    "            print(f\"Downloading {file_name} ...\")\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, file_name)\n",
    "                print(f\"Saved: {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {file_name}: {e}\")\n",
    "\n",
    "print(\"Download complete.\")\n",
    "\n",
    "# Combine the CSV files of each NTL for each station\n",
    "# Loop through each station folder\n",
    "for station in os.listdir(output_dir):\n",
    "    station_path = os.path.join(output_dir, station)\n",
    "\n",
    "    if os.path.isdir(station_path):  # Ensure it's a directory\n",
    "        print(f\"Processing station: {station}\")\n",
    "\n",
    "        for dataset in [\"NTAL\", \"NTOL\", \"HYDL\"]:\n",
    "            output_file = os.path.join(station_path, f\"{station}_{dataset}_combined.csv\")\n",
    "\n",
    "            # Find all CSV files for the dataset type (e.g., \"*_NTAL_*.csv\")\n",
    "            csv_files = glob(os.path.join(station_path, f\"*_{dataset}_*.csv\"))\n",
    "\n",
    "            # Handle missing files\n",
    "            if not csv_files:\n",
    "                print(f\"No {dataset} files found for {station}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Read and concatenate all CSVs for this dataset type\n",
    "            dfs = [pd.read_csv(f) for f in csv_files]\n",
    "            combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "            # Sort by time if needed (assuming time is the first column)\n",
    "            combined_df.sort_values(by=combined_df.columns[0], inplace=True)\n",
    "\n",
    "            # Save the merged file\n",
    "            combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Merging complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bede8c3-da9c-4a83-b299-2978ec04f206",
   "metadata": {},
   "source": [
    "# GNSS data\n",
    "This section describes the preprocessing of uncorrected GNSS displacement data from multiple datasets. Ensure that the file paths are correct before running the code. The datasets included are:\n",
    "\n",
    "1. **GR dataset** – Combines four solutions. Data available [here](https://doi.pangaea.de/10.1594/PANGAEA.967516).\n",
    "2. **NGL dataset** – Available for SYOG and VESL stations. Data available [here](https://geodesy.unr.edu/NGLStationPages/stations/).\n",
    "3. **TUD and OSU datasets** – Two additional solutions from the GR dataset obtained via personal communication.\n",
    "5. **AY dataset** – In-house processed data.\n",
    "\n",
    "\n",
    "For each dataset, the following steps are applied:\n",
    "\n",
    "- **Data loading** – Read CSV, TXT, or JSON files and assign standard column names.\n",
    "- **Unit conversion** – Convert displacement values from meters to millimeters where necessary.\n",
    "- **Time handling** – Convert dates to `datetime` objects, compute Modified Julian Dates (MJD), and filter the data between February 2003 and December 2023.\n",
    "- **Displacement processing** – For each station, displacements are normalized relative to the first epoch and transformed into local East-North-Up (ENU) components using `process_station_displacements`.\n",
    "\n",
    "This preprocessing ensures that all GNSS time series are harmonized in units, time reference, and local coordinate representation, ready for subsequent analysis and the application of non-tidal loading corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7b7d4f-e4c0-48a4-8ecd-44c780abdf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>std_x</th>\n",
       "      <th>std_y</th>\n",
       "      <th>std_z</th>\n",
       "      <th>corr_xy</th>\n",
       "      <th>corr_xz</th>\n",
       "      <th>corr_yz</th>\n",
       "      <th>dis_n</th>\n",
       "      <th>dis_e</th>\n",
       "      <th>dis_u</th>\n",
       "      <th>MJD</th>\n",
       "      <th>dX</th>\n",
       "      <th>dY</th>\n",
       "      <th>dZ</th>\n",
       "      <th>east</th>\n",
       "      <th>north</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-01</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.00234</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>0.00444</td>\n",
       "      <td>0.50032</td>\n",
       "      <td>-0.61248</td>\n",
       "      <td>-0.58647</td>\n",
       "      <td>20.80</td>\n",
       "      <td>-33.60</td>\n",
       "      <td>28.77</td>\n",
       "      <td>52671.5</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.00396</td>\n",
       "      <td>0.00274</td>\n",
       "      <td>0.00677</td>\n",
       "      <td>0.47935</td>\n",
       "      <td>-0.67041</td>\n",
       "      <td>-0.59432</td>\n",
       "      <td>22.19</td>\n",
       "      <td>-33.29</td>\n",
       "      <td>20.48</td>\n",
       "      <td>52672.5</td>\n",
       "      <td>-0.00148</td>\n",
       "      <td>-0.00082</td>\n",
       "      <td>0.00824</td>\n",
       "      <td>0.311095</td>\n",
       "      <td>1.399275</td>\n",
       "      <td>-8.288887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-02-03</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.00189</td>\n",
       "      <td>0.00471</td>\n",
       "      <td>0.51028</td>\n",
       "      <td>-0.61594</td>\n",
       "      <td>-0.59748</td>\n",
       "      <td>20.20</td>\n",
       "      <td>-34.01</td>\n",
       "      <td>24.50</td>\n",
       "      <td>52673.5</td>\n",
       "      <td>-0.00135</td>\n",
       "      <td>-0.00164</td>\n",
       "      <td>0.00377</td>\n",
       "      <td>-0.403711</td>\n",
       "      <td>-0.596410</td>\n",
       "      <td>-4.266886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-02-04</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.00237</td>\n",
       "      <td>0.00598</td>\n",
       "      <td>0.43086</td>\n",
       "      <td>-0.64941</td>\n",
       "      <td>-0.58272</td>\n",
       "      <td>21.21</td>\n",
       "      <td>-34.61</td>\n",
       "      <td>18.69</td>\n",
       "      <td>52674.5</td>\n",
       "      <td>-0.00185</td>\n",
       "      <td>-0.00283</td>\n",
       "      <td>0.00956</td>\n",
       "      <td>-1.002234</td>\n",
       "      <td>0.410172</td>\n",
       "      <td>-10.082276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-02-05</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.00238</td>\n",
       "      <td>0.00189</td>\n",
       "      <td>0.00471</td>\n",
       "      <td>0.53215</td>\n",
       "      <td>-0.61119</td>\n",
       "      <td>-0.63715</td>\n",
       "      <td>21.67</td>\n",
       "      <td>-34.68</td>\n",
       "      <td>24.19</td>\n",
       "      <td>52675.5</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>-0.00136</td>\n",
       "      <td>0.00459</td>\n",
       "      <td>-1.080004</td>\n",
       "      <td>0.871288</td>\n",
       "      <td>-4.581992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date             x             y             z    std_x    std_y  \\\n",
       "0 2003-02-01  1.766208e+06  1.460290e+06 -5.932298e+06  0.00234  0.00176   \n",
       "1 2003-02-02  1.766208e+06  1.460290e+06 -5.932298e+06  0.00396  0.00274   \n",
       "2 2003-02-03  1.766208e+06  1.460290e+06 -5.932298e+06  0.00250  0.00189   \n",
       "3 2003-02-04  1.766208e+06  1.460290e+06 -5.932298e+06  0.00339  0.00237   \n",
       "4 2003-02-05  1.766208e+06  1.460290e+06 -5.932298e+06  0.00238  0.00189   \n",
       "\n",
       "     std_z  corr_xy  corr_xz  corr_yz  dis_n  dis_e  dis_u      MJD       dX  \\\n",
       "0  0.00444  0.50032 -0.61248 -0.58647  20.80 -33.60  28.77  52671.5  0.00000   \n",
       "1  0.00677  0.47935 -0.67041 -0.59432  22.19 -33.29  20.48  52672.5 -0.00148   \n",
       "2  0.00471  0.51028 -0.61594 -0.59748  20.20 -34.01  24.50  52673.5 -0.00135   \n",
       "3  0.00598  0.43086 -0.64941 -0.58272  21.21 -34.61  18.69  52674.5 -0.00185   \n",
       "4  0.00471  0.53215 -0.61119 -0.63715  21.67 -34.68  24.19  52675.5  0.00005   \n",
       "\n",
       "        dY       dZ      east     north         up  \n",
       "0  0.00000  0.00000  0.000000  0.000000   0.000000  \n",
       "1 -0.00082  0.00824  0.311095  1.399275  -8.288887  \n",
       "2 -0.00164  0.00377 -0.403711 -0.596410  -4.266886  \n",
       "3 -0.00283  0.00956 -1.002234  0.410172 -10.082276  \n",
       "4 -0.00136  0.00459 -1.080004  0.871288  -4.581992  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GR\n",
    "\n",
    "column_names = ['date', 'x', 'y', 'z','std_x', 'std_y', 'std_z', 'corr_xy', 'corr_xz', 'corr_yz', 'dis_n', 'dis_e', 'dis_u']\n",
    "\n",
    "aboa_gr = pd.read_csv('GNSS/ABOA_GR.tab', sep='\\t', names=column_names, header=None, skiprows=1)\n",
    "syog_gr = pd.read_csv('GNSS/SYOG_GR.tab', sep='\\t', names=column_names, header=None, skiprows=1)\n",
    "vesl_gr= pd.read_csv('GNSS/VESL_GR.tab', sep='\\t', names=column_names, header=None, skiprows=1)\n",
    "\n",
    "# Convert displacement to mm\n",
    "for i, df in enumerate([aboa_gr, syog_gr, vesl_gr]):\n",
    "    df[['dis_e', 'dis_n', 'dis_u']] *= 1000  # Convert meters to mm\n",
    "    df['MJD'] = df['date'].apply(lambda x: datetime_to_mjd(x) if pd.notna(x) else None)\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Convert to datetime, handle errors\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "    df = df[(df['date'] >= '2003-02-01') & (df['date'] <= '2023-12-31')].copy()\n",
    "\n",
    "    \n",
    "    # Save back to original variable\n",
    "    if i == 0:\n",
    "        aboa_gr = df\n",
    "    elif i == 1:\n",
    "        syog_gr = df\n",
    "    else:\n",
    "        vesl_gr = df\n",
    "\n",
    "# Process displacements for ABOA, SYOG, and VESL\n",
    "aboa_gr = process_station_displacements('ABOA', aboa_gr)\n",
    "syog_gr = process_station_displacements('SYOG', syog_gr)\n",
    "vesl_gr = process_station_displacements('VESL', vesl_gr)\n",
    "\n",
    "syog_gr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ccae29b-5270-4744-92d9-ac7d9ae28049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th>decimal_year</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>std_x</th>\n",
       "      <th>std_y</th>\n",
       "      <th>std_z</th>\n",
       "      <th>corr_xy</th>\n",
       "      <th>corr_yz</th>\n",
       "      <th>corr_xz</th>\n",
       "      <th>antenna_height</th>\n",
       "      <th>MJD</th>\n",
       "      <th>dX</th>\n",
       "      <th>dY</th>\n",
       "      <th>dZ</th>\n",
       "      <th>east</th>\n",
       "      <th>north</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SYOG</td>\n",
       "      <td>2003-02-01</td>\n",
       "      <td>2003.0856</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.598334</td>\n",
       "      <td>-0.575781</td>\n",
       "      <td>-0.644829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52671.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SYOG</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>2003.0883</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.007927</td>\n",
       "      <td>0.650038</td>\n",
       "      <td>-0.602090</td>\n",
       "      <td>-0.728828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52672.0</td>\n",
       "      <td>-0.004314</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>0.019248</td>\n",
       "      <td>7.022677</td>\n",
       "      <td>7.090930</td>\n",
       "      <td>-17.895965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SYOG</td>\n",
       "      <td>2003-02-03</td>\n",
       "      <td>2003.0910</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.006124</td>\n",
       "      <td>0.580927</td>\n",
       "      <td>-0.583285</td>\n",
       "      <td>-0.642008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52673.0</td>\n",
       "      <td>-0.003306</td>\n",
       "      <td>-0.005278</td>\n",
       "      <td>0.006989</td>\n",
       "      <td>-1.961546</td>\n",
       "      <td>-3.014921</td>\n",
       "      <td>-8.642397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SYOG</td>\n",
       "      <td>2003-02-04</td>\n",
       "      <td>2003.0938</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.006665</td>\n",
       "      <td>0.623314</td>\n",
       "      <td>-0.608244</td>\n",
       "      <td>-0.650978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52674.0</td>\n",
       "      <td>-0.008505</td>\n",
       "      <td>-0.006624</td>\n",
       "      <td>0.018677</td>\n",
       "      <td>0.314498</td>\n",
       "      <td>-3.368920</td>\n",
       "      <td>-21.297441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SYOG</td>\n",
       "      <td>2003-02-05</td>\n",
       "      <td>2003.0965</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.005791</td>\n",
       "      <td>0.603978</td>\n",
       "      <td>-0.604915</td>\n",
       "      <td>-0.641631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52675.0</td>\n",
       "      <td>-0.000839</td>\n",
       "      <td>-0.003468</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>-2.137691</td>\n",
       "      <td>-1.498268</td>\n",
       "      <td>-4.069134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station       date  decimal_year             x             y             z  \\\n",
       "0    SYOG 2003-02-01     2003.0856  1.766208e+06  1.460290e+06 -5.932298e+06   \n",
       "1    SYOG 2003-02-02     2003.0883  1.766208e+06  1.460290e+06 -5.932298e+06   \n",
       "2    SYOG 2003-02-03     2003.0910  1.766208e+06  1.460290e+06 -5.932298e+06   \n",
       "3    SYOG 2003-02-04     2003.0938  1.766208e+06  1.460290e+06 -5.932298e+06   \n",
       "4    SYOG 2003-02-05     2003.0965  1.766208e+06  1.460290e+06 -5.932298e+06   \n",
       "\n",
       "      std_x     std_y     std_z   corr_xy   corr_yz   corr_xz  antenna_height  \\\n",
       "0  0.001941  0.001730  0.006017  0.598334 -0.575781 -0.644829             0.0   \n",
       "1  0.003233  0.002351  0.007927  0.650038 -0.602090 -0.728828             0.0   \n",
       "2  0.001977  0.001815  0.006124  0.580927 -0.583285 -0.642008             0.0   \n",
       "3  0.002338  0.001969  0.006665  0.623314 -0.608244 -0.650978             0.0   \n",
       "4  0.001882  0.001646  0.005791  0.603978 -0.604915 -0.641631             0.0   \n",
       "\n",
       "       MJD        dX        dY        dZ      east     north         up  \n",
       "0  52671.0  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
       "1  52672.0 -0.004314  0.005545  0.019248  7.022677  7.090930 -17.895965  \n",
       "2  52673.0 -0.003306 -0.005278  0.006989 -1.961546 -3.014921  -8.642397  \n",
       "3  52674.0 -0.008505 -0.006624  0.018677  0.314498 -3.368920 -21.297441  \n",
       "4  52675.0 -0.000839 -0.003468  0.003262 -2.137691 -1.498268  -4.069134  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NGL\n",
    "# Column names\n",
    "columns = ['station', 'date', 'decimal_year', 'x', 'y', 'z', 'std_x', 'std_y', 'std_z', 'corr_xy', 'corr_yz', 'corr_xz', 'antenna_height']\n",
    "\n",
    "syog_ngl = pd.read_csv('GNSS/SYOG_NGL.txyz2.txt', sep='\\\\s+', names=columns)\n",
    "vesl_ngl = pd.read_csv('GNSS/VESL_NGL.txyz2.txt', sep='\\\\s+', names=columns)\n",
    "\n",
    "for i, df in enumerate([syog_ngl, vesl_ngl]):\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%y%b%d', errors='coerce')  # Convert to datetime, handle errors\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "    df['MJD'] = df['date'].apply(lambda x: datetime_to_mjd(x) if pd.notna(x) else None)\n",
    "    df = df[(df['date'] >= '2003-02-01') & (df['date'] <= '2023-12-31')].copy()\n",
    "\n",
    "    # Save back to original variable\n",
    "    if i == 0:\n",
    "        syog_ngl = df\n",
    "    else:\n",
    "        vesl_ngl = df\n",
    "        \n",
    "# Process displacements for SYOG and VESL\n",
    "syog_ngl = process_station_displacements('SYOG', syog_ngl)\n",
    "vesl_ngl = process_station_displacements('VESL', vesl_ngl)\n",
    "\n",
    "syog_ngl.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f905730c-d164-4a52-b5d4-1a713c9ab115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>std_X</th>\n",
       "      <th>std_Y</th>\n",
       "      <th>std_Z</th>\n",
       "      <th>cor_XY</th>\n",
       "      <th>cor_XZ</th>\n",
       "      <th>cor_YZ</th>\n",
       "      <th>weight</th>\n",
       "      <th>MJD</th>\n",
       "      <th>dX</th>\n",
       "      <th>dY</th>\n",
       "      <th>dZ</th>\n",
       "      <th>east</th>\n",
       "      <th>north</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66006S002</td>\n",
       "      <td>2003-02-01</td>\n",
       "      <td>W</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.517034</td>\n",
       "      <td>-0.556598</td>\n",
       "      <td>-0.604015</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>52671.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66006S002</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>W</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.519745</td>\n",
       "      <td>-0.573416</td>\n",
       "      <td>-0.615539</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>52672.0</td>\n",
       "      <td>-0.00025</td>\n",
       "      <td>-0.00034</td>\n",
       "      <td>-0.00070</td>\n",
       "      <td>-0.102735</td>\n",
       "      <td>-0.632933</td>\n",
       "      <td>0.506895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66006S002</td>\n",
       "      <td>2003-02-03</td>\n",
       "      <td>W</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.529202</td>\n",
       "      <td>-0.576091</td>\n",
       "      <td>-0.606027</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>52673.0</td>\n",
       "      <td>0.00059</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>0.00066</td>\n",
       "      <td>-0.137036</td>\n",
       "      <td>0.845398</td>\n",
       "      <td>-0.382522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66006S002</td>\n",
       "      <td>2003-02-04</td>\n",
       "      <td>W</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.507924</td>\n",
       "      <td>-0.631761</td>\n",
       "      <td>-0.541302</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>52674.0</td>\n",
       "      <td>-0.00280</td>\n",
       "      <td>-0.00270</td>\n",
       "      <td>0.01219</td>\n",
       "      <td>-0.296699</td>\n",
       "      <td>0.746158</td>\n",
       "      <td>-12.770329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66006S002</td>\n",
       "      <td>2003-02-05</td>\n",
       "      <td>W</td>\n",
       "      <td>1.766208e+06</td>\n",
       "      <td>1.460290e+06</td>\n",
       "      <td>-5.932298e+06</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.548538</td>\n",
       "      <td>-0.580387</td>\n",
       "      <td>-0.686026</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>52675.0</td>\n",
       "      <td>-0.00220</td>\n",
       "      <td>0.00128</td>\n",
       "      <td>-0.00191</td>\n",
       "      <td>2.388340</td>\n",
       "      <td>-1.505766</td>\n",
       "      <td>1.467992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     station       date flag             x             y             z  \\\n",
       "0  66006S002 2003-02-01    W  1.766208e+06  1.460290e+06 -5.932298e+06   \n",
       "1  66006S002 2003-02-02    W  1.766208e+06  1.460290e+06 -5.932298e+06   \n",
       "2  66006S002 2003-02-03    W  1.766208e+06  1.460290e+06 -5.932298e+06   \n",
       "3  66006S002 2003-02-04    W  1.766208e+06  1.460290e+06 -5.932298e+06   \n",
       "4  66006S002 2003-02-05    W  1.766208e+06  1.460290e+06 -5.932298e+06   \n",
       "\n",
       "      std_X     std_Y     std_Z    cor_XY    cor_XZ    cor_YZ  weight  \\\n",
       "0  0.000249  0.000240  0.000818  0.517034 -0.556598 -0.604015  0.0011   \n",
       "1  0.000339  0.000323  0.001140  0.519745 -0.573416 -0.615539  0.0012   \n",
       "2  0.000267  0.000255  0.000887  0.529202 -0.576091 -0.606027  0.0012   \n",
       "3  0.000322  0.000257  0.000989  0.507924 -0.631761 -0.541302  0.0012   \n",
       "4  0.000269  0.000296  0.001013  0.548538 -0.580387 -0.686026  0.0011   \n",
       "\n",
       "       MJD       dX       dY       dZ      east     north         up  \n",
       "0  52671.0  0.00000  0.00000  0.00000  0.000000  0.000000   0.000000  \n",
       "1  52672.0 -0.00025 -0.00034 -0.00070 -0.102735 -0.632933   0.506895  \n",
       "2  52673.0  0.00059  0.00031  0.00066 -0.137036  0.845398  -0.382522  \n",
       "3  52674.0 -0.00280 -0.00270  0.01219 -0.296699  0.746158 -12.770329  \n",
       "4  52675.0 -0.00220  0.00128 -0.00191  2.388340 -1.505766   1.467992  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TUD\n",
    "# Column names\n",
    "column_names = ['station', 'date', 'flag', 'x', 'y', 'z', 'std_X', 'std_Y', 'std_Z','cor_XY', 'cor_XZ', 'cor_YZ', 'weight']\n",
    "\n",
    "aboa_tud = pd.read_csv('GNSS/ABOA_TUD.xyz', sep='\\\\s+', names=column_names, skiprows=7)\n",
    "syog_tud = pd.read_csv('GNSS/SYOG_TUD.xyz', sep='\\\\s+', names=column_names, skiprows=7)\n",
    "vesl_tud = pd.read_csv('GNSS/VESL_TUD.xyz', sep='\\\\s+', names=column_names, skiprows=7)\n",
    "\n",
    "# Convert date to MJD and filter by date range\n",
    "for i, df in enumerate([aboa_tud, syog_tud, vesl_tud]):\n",
    "    df['MJD'] = df['date'].apply(lambda x: datetime_to_mjd(x) if pd.notna(x) else None)\n",
    "    df = df[(df['date'] >= '2003-02-01') & (df['date'] <= '2023-12-31')].copy()\n",
    "    \n",
    "    # Save back to original variable\n",
    "    if i == 0:\n",
    "        aboa_tud = df\n",
    "    elif i == 1:\n",
    "        syog_tud = df\n",
    "    else:\n",
    "        vesl_tud = df\n",
    "\n",
    "# Process displacements for ABOA, SYOG, and VESL\n",
    "aboa_tud = process_station_displacements('ABOA', aboa_tud)\n",
    "syog_tud = process_station_displacements('SYOG', syog_tud)\n",
    "vesl_tud = process_station_displacements('VESL', vesl_tud)\n",
    "\n",
    "\n",
    "syog_tud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ea7271-fa49-4cf1-b0e8-093abcd92233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MJD</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>n</th>\n",
       "      <th>e</th>\n",
       "      <th>u</th>\n",
       "      <th>date</th>\n",
       "      <th>dX</th>\n",
       "      <th>dY</th>\n",
       "      <th>dZ</th>\n",
       "      <th>east</th>\n",
       "      <th>north</th>\n",
       "      <th>up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52671</td>\n",
       "      <td>2.009330e+06</td>\n",
       "      <td>-99741.472907</td>\n",
       "      <td>-6.033158e+06</td>\n",
       "      <td>-53.078679</td>\n",
       "      <td>3.262847</td>\n",
       "      <td>-21.540744</td>\n",
       "      <td>2003-02-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52672</td>\n",
       "      <td>2.009330e+06</td>\n",
       "      <td>-99741.472502</td>\n",
       "      <td>-6.033158e+06</td>\n",
       "      <td>-53.939590</td>\n",
       "      <td>3.603221</td>\n",
       "      <td>-23.144999</td>\n",
       "      <td>2003-02-02</td>\n",
       "      <td>-0.001303</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.340374</td>\n",
       "      <td>-0.860911</td>\n",
       "      <td>-1.604255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52673</td>\n",
       "      <td>2.009330e+06</td>\n",
       "      <td>-99741.473042</td>\n",
       "      <td>-6.033158e+06</td>\n",
       "      <td>-54.961172</td>\n",
       "      <td>2.973860</td>\n",
       "      <td>-25.745178</td>\n",
       "      <td>2003-02-03</td>\n",
       "      <td>-0.003120</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>-0.288987</td>\n",
       "      <td>-1.882494</td>\n",
       "      <td>-4.204434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52674</td>\n",
       "      <td>2.009330e+06</td>\n",
       "      <td>-99741.472225</td>\n",
       "      <td>-6.033158e+06</td>\n",
       "      <td>-55.338377</td>\n",
       "      <td>3.737949</td>\n",
       "      <td>-28.030159</td>\n",
       "      <td>2003-02-04</td>\n",
       "      <td>-0.004157</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.475102</td>\n",
       "      <td>-2.259698</td>\n",
       "      <td>-6.489415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52675</td>\n",
       "      <td>2.009330e+06</td>\n",
       "      <td>-99741.473582</td>\n",
       "      <td>-6.033158e+06</td>\n",
       "      <td>-53.863946</td>\n",
       "      <td>2.454582</td>\n",
       "      <td>-27.659167</td>\n",
       "      <td>2003-02-05</td>\n",
       "      <td>-0.002706</td>\n",
       "      <td>-0.000675</td>\n",
       "      <td>0.005561</td>\n",
       "      <td>-0.808265</td>\n",
       "      <td>-0.785267</td>\n",
       "      <td>-6.118423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MJD             x             y             z          n         e  \\\n",
       "0  52671  2.009330e+06 -99741.472907 -6.033158e+06 -53.078679  3.262847   \n",
       "1  52672  2.009330e+06 -99741.472502 -6.033158e+06 -53.939590  3.603221   \n",
       "2  52673  2.009330e+06 -99741.473042 -6.033158e+06 -54.961172  2.973860   \n",
       "3  52674  2.009330e+06 -99741.472225 -6.033158e+06 -55.338377  3.737949   \n",
       "4  52675  2.009330e+06 -99741.473582 -6.033158e+06 -53.863946  2.454582   \n",
       "\n",
       "           u       date        dX        dY        dZ      east     north  \\\n",
       "0 -21.540744 2003-02-01  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1 -23.144999 2003-02-02 -0.001303  0.000405  0.001252  0.340374 -0.860911   \n",
       "2 -25.745178 2003-02-03 -0.003120 -0.000134  0.003399 -0.288987 -1.882494   \n",
       "3 -28.030159 2003-02-04 -0.004157  0.000682  0.005450  0.475102 -2.259698   \n",
       "4 -27.659167 2003-02-05 -0.002706 -0.000675  0.005561 -0.808265 -0.785267   \n",
       "\n",
       "         up  \n",
       "0  0.000000  \n",
       "1 -1.604255  \n",
       "2 -4.204434  \n",
       "3 -6.489415  \n",
       "4 -6.118423  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OSU\n",
    "\n",
    "# Function to load and process time_series from a JSON file\n",
    "def load_time_series_json(filepath):\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "    ts = data['time_series']\n",
    "    return  pd.DataFrame({key: ts[key] for key in ['mjd', 'x', 'y', 'z', 'n', 'e', 'u'] if len(ts[key]) == len(ts['mjd'])})\n",
    "\n",
    "# Load time series data for ABOA, SYOG, and VESL\n",
    "aboa_osu = load_time_series_json('GNSS/ABOA_OSU.json')\n",
    "syog_osu = load_time_series_json('GNSS/SYOG_OSU.json')\n",
    "vesl_osu = load_time_series_json('GNSS/VESL_OSU.json')\n",
    "\n",
    "# Convert displacement to mm\n",
    "for i, df in enumerate([aboa_osu, syog_osu, vesl_osu]):\n",
    "    df[['e', 'n', 'u']] *= 1000  # Convert meters to mm\n",
    "    df.rename(columns={'mjd': 'MJD'}, inplace=True)\n",
    "    df['date'] = df['MJD'].apply(lambda x: mjd_to_datetime(x) if pd.notna(x) else None)\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Convert to datetime, handle errors\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "    df = df[(df['date'] >= '2003-02-01') & (df['date'] <= '2023-12-31')].copy()\n",
    "    \n",
    "    # Save back to original variable\n",
    "    if i == 0:\n",
    "        aboa_osu = df\n",
    "    elif i == 1:\n",
    "        syog_osu = df\n",
    "    else:\n",
    "        vesl_osu = df\n",
    "\n",
    "# Process displacements for ABOA, SYOG, and VESL\n",
    "aboa_osu = process_station_displacements('ABOA', aboa_osu)\n",
    "syog_osu = process_station_displacements('SYOG', syog_osu)\n",
    "vesl_osu = process_station_displacements('VESL', vesl_osu)\n",
    "\n",
    "vesl_osu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70bcebf0-1d0f-44d5-9afc-e91858bba7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>decimal-year</th>\n",
       "      <th>east</th>\n",
       "      <th>north</th>\n",
       "      <th>up</th>\n",
       "      <th>sigmaE</th>\n",
       "      <th>sigmaN</th>\n",
       "      <th>sigmaU</th>\n",
       "      <th>corrEN</th>\n",
       "      <th>corrEU</th>\n",
       "      <th>corrNU</th>\n",
       "      <th>julian-sec</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>MJD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2003-02-01</td>\n",
       "      <td>2003.085567</td>\n",
       "      <td>33.302</td>\n",
       "      <td>-27.527</td>\n",
       "      <td>1.286</td>\n",
       "      <td>1.782</td>\n",
       "      <td>2.590</td>\n",
       "      <td>8.914</td>\n",
       "      <td>-0.167028</td>\n",
       "      <td>-0.082621</td>\n",
       "      <td>-0.090367</td>\n",
       "      <td>97373100.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52671.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>2003-02-05</td>\n",
       "      <td>2003.096500</td>\n",
       "      <td>33.172</td>\n",
       "      <td>-25.718</td>\n",
       "      <td>-1.838</td>\n",
       "      <td>1.857</td>\n",
       "      <td>2.787</td>\n",
       "      <td>9.862</td>\n",
       "      <td>-0.084776</td>\n",
       "      <td>-0.076096</td>\n",
       "      <td>-0.042264</td>\n",
       "      <td>97718100.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52675.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>2003-02-11</td>\n",
       "      <td>2003.112936</td>\n",
       "      <td>28.484</td>\n",
       "      <td>-27.921</td>\n",
       "      <td>-2.754</td>\n",
       "      <td>1.701</td>\n",
       "      <td>2.593</td>\n",
       "      <td>9.609</td>\n",
       "      <td>0.024209</td>\n",
       "      <td>-0.187838</td>\n",
       "      <td>-0.149175</td>\n",
       "      <td>98236800.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52681.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>2003-02-12</td>\n",
       "      <td>2003.115665</td>\n",
       "      <td>30.818</td>\n",
       "      <td>-25.187</td>\n",
       "      <td>-10.220</td>\n",
       "      <td>1.778</td>\n",
       "      <td>2.894</td>\n",
       "      <td>9.340</td>\n",
       "      <td>-0.054947</td>\n",
       "      <td>-0.128033</td>\n",
       "      <td>-0.091937</td>\n",
       "      <td>98322900.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52682.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>2003-02-13</td>\n",
       "      <td>2003.118403</td>\n",
       "      <td>31.007</td>\n",
       "      <td>-24.585</td>\n",
       "      <td>-1.635</td>\n",
       "      <td>1.647</td>\n",
       "      <td>2.367</td>\n",
       "      <td>9.297</td>\n",
       "      <td>0.021599</td>\n",
       "      <td>-0.092969</td>\n",
       "      <td>-0.136406</td>\n",
       "      <td>98409300.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52683.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  decimal-year    east   north      up  sigmaE  sigmaN  sigmaU  \\\n",
       "475  2003-02-01   2003.085567  33.302 -27.527   1.286   1.782   2.590   8.914   \n",
       "476  2003-02-05   2003.096500  33.172 -25.718  -1.838   1.857   2.787   9.862   \n",
       "477  2003-02-11   2003.112936  28.484 -27.921  -2.754   1.701   2.593   9.609   \n",
       "478  2003-02-12   2003.115665  30.818 -25.187 -10.220   1.778   2.894   9.340   \n",
       "479  2003-02-13   2003.118403  31.007 -24.585  -1.635   1.647   2.367   9.297   \n",
       "\n",
       "       corrEN    corrEU    corrNU  julian-sec    year  month   day  hour  \\\n",
       "475 -0.167028 -0.082621 -0.090367  97373100.0  2003.0    2.0   1.0  12.0   \n",
       "476 -0.084776 -0.076096 -0.042264  97718100.0  2003.0    2.0   5.0  11.0   \n",
       "477  0.024209 -0.187838 -0.149175  98236800.0  2003.0    2.0  11.0  12.0   \n",
       "478 -0.054947 -0.128033 -0.091937  98322900.0  2003.0    2.0  12.0  11.0   \n",
       "479  0.021599 -0.092969 -0.136406  98409300.0  2003.0    2.0  13.0  11.0   \n",
       "\n",
       "     minute  second      MJD  \n",
       "475     5.0     0.0  52671.5  \n",
       "476    55.0     0.0  52675.5  \n",
       "477     0.0     0.0  52681.5  \n",
       "478    55.0     0.0  52682.5  \n",
       "479    55.0     0.0  52683.5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AY\n",
    "# Column names\n",
    "column_names = ['decimal-year', 'east', 'north', 'up', 'sigmaE', 'sigmaN', 'sigmaU', 'corrEN', 'corrEU', 'corrNU', 'julian-sec', 'year', 'month', 'day', 'hour', 'minute', 'second']\n",
    "\n",
    "aboa_ay = pd.read_csv('GNSS/ABOA_AY.series', sep='\\\\s+', names=column_names)\n",
    "syog_ay = pd.read_csv('GNSS/SYOG_AY.series', sep='\\\\s+', names=column_names)\n",
    "vesl_ay = pd.read_csv('GNSS/VESL_AY.series', sep='\\\\s+', names=column_names)\n",
    "\n",
    "# Convert displacement to mm\n",
    "for i, df in enumerate([aboa_ay, syog_ay, vesl_ay]):\n",
    "    df[['east', 'north', 'up', 'sigmaE', 'sigmaN', 'sigmaU']] *= 1000   # Convert meters to mm\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']].astype(str).agg('-'.join, axis=1) + ' 12:00:00')\n",
    "    \n",
    "    # Group by date first\n",
    "    df = df.groupby('date', as_index=False).mean(numeric_only=True)\n",
    "    \n",
    "    # MJD and date formatting\n",
    "    df['MJD'] = df['date'].apply(lambda x: datetime_to_mjd(x) if pd.notna(x) else None)\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "    df = df[(df['date'] >= '2003-02-01') & (df['date'] <= '2023-12-31')].copy()\n",
    " \n",
    "    # Save back to original variable\n",
    "    if i == 0:\n",
    "        aboa_ay = df\n",
    "    elif i == 1:\n",
    "        syog_ay = df\n",
    "    else:\n",
    "        vesl_ay = df\n",
    "\n",
    "\n",
    "syog_ay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a827e5-ed47-42e8-86a9-036296093213",
   "metadata": {},
   "source": [
    "# NTL data\n",
    "This section preprocesses NTL data from **EOST** and **ESMGFZ**. The workflow is as follows:\n",
    "1. Daily NTL displacements are loaded for each station.\n",
    "2. Dates are converted to datetime, and values to millimeters if necessary.\n",
    "3. Multiple daily measurements are averaged to obtain a single daily value per component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e2589f4-900a-41a5-a172-6ae5174f2626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MJD</th>\n",
       "      <th>east_ib</th>\n",
       "      <th>north_ib</th>\n",
       "      <th>up_ib</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43874.000000</td>\n",
       "      <td>-1.622</td>\n",
       "      <td>0.437</td>\n",
       "      <td>-4.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43874.041667</td>\n",
       "      <td>-1.605</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-4.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43874.083333</td>\n",
       "      <td>-1.583</td>\n",
       "      <td>0.419</td>\n",
       "      <td>-4.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43874.125000</td>\n",
       "      <td>-1.570</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-4.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43874.166667</td>\n",
       "      <td>-1.547</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-4.231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            MJD  east_ib  north_ib  up_ib\n",
       "0  43874.000000   -1.622     0.437 -4.722\n",
       "1  43874.041667   -1.605     0.436 -4.605\n",
       "2  43874.083333   -1.583     0.419 -4.462\n",
       "3  43874.125000   -1.570     0.397 -4.357\n",
       "4  43874.166667   -1.547     0.373 -4.231"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EOST\n",
    "# Create dataframes\n",
    "aboa_era5ib = pd.read_csv('EOST/ABOA_era5ib.txt', sep='\\\\s+', names=['MJD', 'east_ib', 'north_ib', 'up_ib'])\n",
    "aboa_era5tugo = pd.read_csv('EOST/ABOA_era5tugo.txt', sep='\\\\s+', names=['MJD', 'east_tugo', 'north_tugo', 'up_tugo'])\n",
    "aboa_ecco2 = pd.read_csv('EOST/ABOA_ecco2.txt', sep='\\\\s+', names=['MJD', 'east_ecco2', 'north_ecco2', 'up_ecco2'])\n",
    "aboa_merra2 = pd.read_csv('EOST/ABOA_merra2.txt', sep='\\\\s+', names=['MJD', 'east_merra2', 'north_merra2', 'up_merra2'])\n",
    "\n",
    "syog_era5ib = pd.read_csv('EOST/SYOG_era5ib.txt', sep='\\\\s+', names=['MJD', 'east_ib', 'north_ib', 'up_ib'])\n",
    "syog_era5tugo = pd.read_csv('EOST/SYOG_era5tugo.txt', sep='\\\\s+', names=['MJD', 'east_tugo', 'north_tugo', 'up_tugo'])\n",
    "syog_ecco2 = pd.read_csv('EOST/SYOG_ecco2.txt', sep='\\\\s+', names=['MJD', 'east_ecco2', 'north_ecco2', 'up_ecco2'])\n",
    "syog_merra2 = pd.read_csv('EOST/SYOG_merra2.txt', sep='\\\\s+', names=['MJD', 'east_merra2', 'north_merra2', 'up_merra2'])\n",
    "\n",
    "vesl_era5ib = pd.read_csv('EOST/VESL_era5ib.txt', sep='\\\\s+', names=['MJD', 'east_ib', 'north_ib', 'up_ib'])\n",
    "vesl_era5tugo = pd.read_csv('EOST/VESL_era5tugo.txt', sep='\\\\s+', names=['MJD', 'east_tugo', 'north_tugo', 'up_tugo'])\n",
    "vesl_ecco2 = pd.read_csv('EOST/VESL_ecco2.txt', sep='\\\\s+', names=['MJD', 'east_ecco2', 'north_ecco2', 'up_ecco2'])\n",
    "vesl_merra2 = pd.read_csv('EOST/VESL_merra2.txt', sep='\\\\s+', names=['MJD', 'east_merra2', 'north_merra2', 'up_merra2'])\n",
    "\n",
    "syog_era5ib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98ab1838-6ba6-4c71-a7af-249c20042304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MJD</th>\n",
       "      <th>east_ib</th>\n",
       "      <th>north_ib</th>\n",
       "      <th>up_ib</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43874.479167</td>\n",
       "      <td>-1.033375</td>\n",
       "      <td>0.314417</td>\n",
       "      <td>-4.557375</td>\n",
       "      <td>1979-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43875.479167</td>\n",
       "      <td>-0.668333</td>\n",
       "      <td>0.278625</td>\n",
       "      <td>-2.239875</td>\n",
       "      <td>1979-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43876.479167</td>\n",
       "      <td>-0.671958</td>\n",
       "      <td>0.261292</td>\n",
       "      <td>-2.374833</td>\n",
       "      <td>1979-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43877.479167</td>\n",
       "      <td>-0.591375</td>\n",
       "      <td>0.115167</td>\n",
       "      <td>-2.043875</td>\n",
       "      <td>1979-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43878.479167</td>\n",
       "      <td>-0.670292</td>\n",
       "      <td>0.124875</td>\n",
       "      <td>-3.004958</td>\n",
       "      <td>1979-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            MJD   east_ib  north_ib     up_ib        date\n",
       "0  43874.479167 -1.033375  0.314417 -4.557375  1979-01-01\n",
       "1  43875.479167 -0.668333  0.278625 -2.239875  1979-01-02\n",
       "2  43876.479167 -0.671958  0.261292 -2.374833  1979-01-03\n",
       "3  43877.479167 -0.591375  0.115167 -2.043875  1979-01-04\n",
       "4  43878.479167 -0.670292  0.124875 -3.004958  1979-01-05"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert MJD to datetime \n",
    "for df in [aboa_era5ib , aboa_era5tugo, aboa_ecco2, aboa_merra2,\n",
    "          syog_era5ib , syog_era5tugo, syog_ecco2, syog_merra2,\n",
    "          vesl_era5ib , vesl_era5tugo, vesl_ecco2, vesl_merra2] :\n",
    "    df['date'] = df['MJD'].apply(lambda x: mjd_to_datetime(x) if pd.notna(x) else None)\n",
    "\n",
    "# Calculate mean for NTL since there are multiple measurements per day\n",
    "dataframes = [aboa_era5ib, aboa_era5tugo, aboa_ecco2, aboa_merra2,\n",
    "              syog_era5ib, syog_era5tugo, syog_ecco2, syog_merra2,\n",
    "              vesl_era5ib, vesl_era5tugo, vesl_ecco2, vesl_merra2]\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    df = df.groupby(df['date'].dt.date, as_index=False).mean()\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "    dataframes[i] = df\n",
    "\n",
    "# Unpack them back to original variables\n",
    "(aboa_era5ib, aboa_era5tugo, aboa_ecco2, aboa_merra2,\n",
    " syog_era5ib, syog_era5tugo, syog_ecco2, syog_merra2,\n",
    " vesl_era5ib, vesl_era5tugo, vesl_ecco2, vesl_merra2) = dataframes\n",
    "\n",
    "\n",
    "aboa_era5ib.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "356db5a0-2c45-4194-9ad0-dcf31260fc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>east_ntal</th>\n",
       "      <th>north_ntal</th>\n",
       "      <th>up_ntal</th>\n",
       "      <th>MJD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1995-01-01 00:00:00</td>\n",
       "      <td>-73.043771</td>\n",
       "      <td>-13.407135</td>\n",
       "      <td>0.195832</td>\n",
       "      <td>-0.409075</td>\n",
       "      <td>-0.140590</td>\n",
       "      <td>49718.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995-01-01 03:00:00</td>\n",
       "      <td>-73.043771</td>\n",
       "      <td>-13.407135</td>\n",
       "      <td>0.246023</td>\n",
       "      <td>-0.488023</td>\n",
       "      <td>-0.521849</td>\n",
       "      <td>49718.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995-01-01 06:00:00</td>\n",
       "      <td>-73.043771</td>\n",
       "      <td>-13.407135</td>\n",
       "      <td>0.264395</td>\n",
       "      <td>-0.515332</td>\n",
       "      <td>-0.639675</td>\n",
       "      <td>49718.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995-01-01 09:00:00</td>\n",
       "      <td>-73.043771</td>\n",
       "      <td>-13.407135</td>\n",
       "      <td>0.273604</td>\n",
       "      <td>-0.549885</td>\n",
       "      <td>-0.782826</td>\n",
       "      <td>49718.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1995-01-01 12:00:00</td>\n",
       "      <td>-73.043771</td>\n",
       "      <td>-13.407135</td>\n",
       "      <td>0.272998</td>\n",
       "      <td>-0.582403</td>\n",
       "      <td>-1.000604</td>\n",
       "      <td>49718.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date        lat        lon  east_ntal  north_ntal   up_ntal  \\\n",
       "0 1995-01-01 00:00:00 -73.043771 -13.407135   0.195832   -0.409075 -0.140590   \n",
       "1 1995-01-01 03:00:00 -73.043771 -13.407135   0.246023   -0.488023 -0.521849   \n",
       "2 1995-01-01 06:00:00 -73.043771 -13.407135   0.264395   -0.515332 -0.639675   \n",
       "3 1995-01-01 09:00:00 -73.043771 -13.407135   0.273604   -0.549885 -0.782826   \n",
       "4 1995-01-01 12:00:00 -73.043771 -13.407135   0.272998   -0.582403 -1.000604   \n",
       "\n",
       "         MJD  \n",
       "0  49718.000  \n",
       "1  49718.125  \n",
       "2  49718.250  \n",
       "3  49718.375  \n",
       "4  49718.500  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ESMGFZ\n",
    "# Function to read csv, convert to mm, and add MJD\n",
    "def read_and_convert(file_path, column_names):\n",
    "    df = pd.read_csv(file_path, sep=',', names=column_names, skiprows=1)\n",
    "    \n",
    "    # Ensure last three columns are numeric before multiplying\n",
    "    df.iloc[:, -3:] = df.iloc[:, -3:].apply(pd.to_numeric, errors='coerce') * 1000  \n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "    df['MJD'] = df['date'].apply(datetime_to_mjd)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Column names for each dataset\n",
    "column_names_ntal = ['date', 'lat', 'lon', 'east_ntal', 'north_ntal', 'up_ntal']\n",
    "column_names_ntol = ['date', 'lat', 'lon', 'east_ntol', 'north_ntol', 'up_ntol']\n",
    "column_names_hydl = ['date', 'lat', 'lon', 'east_hydl', 'north_hydl', 'up_hydl']\n",
    "\n",
    "# Read and process data\n",
    "aboa_ntal = read_and_convert('ESMGFZ/ABOA/ABOA_NTAL_combined.csv', column_names_ntal)\n",
    "aboa_ntol = read_and_convert('ESMGFZ/ABOA/ABOA_NTOL_combined.csv', column_names_ntol)\n",
    "aboa_hydl = read_and_convert('ESMGFZ/ABOA/ABOA_HYDL_combined.csv', column_names_hydl)\n",
    "\n",
    "syog_ntal = read_and_convert('ESMGFZ/SYOG/SYOG_NTAL_combined.csv', column_names_ntal)\n",
    "syog_ntol = read_and_convert('ESMGFZ/SYOG/SYOG_NTOL_combined.csv', column_names_ntol)\n",
    "syog_hydl = read_and_convert('ESMGFZ/SYOG/SYOG_HYDL_combined.csv', column_names_hydl)\n",
    "\n",
    "vesl_ntal = read_and_convert('ESMGFZ/VESL/VESL_NTAL_combined.csv', column_names_ntal)\n",
    "vesl_ntol = read_and_convert('ESMGFZ/VESL/VESL_NTOL_combined.csv', column_names_ntol)\n",
    "vesl_hydl = read_and_convert('ESMGFZ/VESL/VESL_HYDL_combined.csv', column_names_hydl)\n",
    "\n",
    "aboa_ntal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9534ddfb-ea0b-488d-82ee-1dc99951acba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>east_ntal</th>\n",
       "      <th>north_ntal</th>\n",
       "      <th>up_ntal</th>\n",
       "      <th>MJD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>-73.043771</td>\n",
       "      <td>-13.407135</td>\n",
       "      <td>0.245344</td>\n",
       "      <td>-0.540710</td>\n",
       "      <td>-0.795427</td>\n",
       "      <td>49718.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995-01-02</td>\n",
       "      <td>-73.043771</td>\n",
       "      <td>-13.407135</td>\n",
       "      <td>0.198378</td>\n",
       "      <td>-0.522821</td>\n",
       "      <td>-1.343760</td>\n",
       "      <td>49719.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995-01-03</td>\n",
       "      <td>-73.043771</td>\n",
       "      <td>-13.407135</td>\n",
       "      <td>0.269387</td>\n",
       "      <td>-0.450088</td>\n",
       "      <td>-1.837074</td>\n",
       "      <td>49720.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995-01-04</td>\n",
       "      <td>-73.043771</td>\n",
       "      <td>-13.407135</td>\n",
       "      <td>0.279554</td>\n",
       "      <td>-0.319965</td>\n",
       "      <td>-0.906681</td>\n",
       "      <td>49721.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1995-01-05</td>\n",
       "      <td>-73.043771</td>\n",
       "      <td>-13.407135</td>\n",
       "      <td>0.508997</td>\n",
       "      <td>-0.288251</td>\n",
       "      <td>-0.868127</td>\n",
       "      <td>49722.4375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        lat        lon  east_ntal  north_ntal   up_ntal  \\\n",
       "0  1995-01-01 -73.043771 -13.407135   0.245344   -0.540710 -0.795427   \n",
       "1  1995-01-02 -73.043771 -13.407135   0.198378   -0.522821 -1.343760   \n",
       "2  1995-01-03 -73.043771 -13.407135   0.269387   -0.450088 -1.837074   \n",
       "3  1995-01-04 -73.043771 -13.407135   0.279554   -0.319965 -0.906681   \n",
       "4  1995-01-05 -73.043771 -13.407135   0.508997   -0.288251 -0.868127   \n",
       "\n",
       "          MJD  \n",
       "0  49718.4375  \n",
       "1  49719.4375  \n",
       "2  49720.4375  \n",
       "3  49721.4375  \n",
       "4  49722.4375  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = [aboa_ntal, aboa_ntol, aboa_hydl,\n",
    "              syog_ntal, syog_ntol, syog_hydl, \n",
    "              vesl_ntal, vesl_ntol, vesl_hydl]\n",
    "\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    df = df.groupby(df['date'].dt.date, as_index=False).mean()\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "    dataframes[i] = df\n",
    "\n",
    "# Unpack them back to original variables\n",
    "(aboa_ntal, aboa_ntol, aboa_hydl,\n",
    " syog_ntal, syog_ntol, syog_hydl, \n",
    " vesl_ntal, vesl_ntol, vesl_hydl) = dataframes\n",
    "\n",
    "aboa_ntal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9fa5ea-4a0e-41f9-ae89-f9de53ee9b4b",
   "metadata": {},
   "source": [
    "# NTL corrections\n",
    "This section applies NTL corrections to the GNSS time series. The procedure involves the following steps:\n",
    "\n",
    "1. **Data merging**: Base GNSS datasets (GR, TUD, OSU, NGL, AY) are merged with relevant auxiliary datasets to enable automated processing.\n",
    "2. **NTL corrections**: Eleven different NTL correction combinations are applied. Some corrections are specific to the GR dataset, which already includes certain NTL adjustments.\n",
    "3. **Saving results**: Corrected time series are stored in a combined dictionary (`merged_all`) and exported as CSV files for each station and dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "958bbff1-868d-4b70-88a4-5ca2d62276f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata for processing\n",
    "base_keys = ['gr', 'tud', 'osu', 'ngl', 'ay']\n",
    "merge_suffix = {\n",
    "    'gr': ['ecco2', 'ntol', 'hydl'],\n",
    "    'tud': ['ib', 'tugo', 'ecco2', 'merra2', 'ntal', 'ntol', 'hydl'],\n",
    "    'osu': ['ib', 'tugo', 'ecco2', 'merra2', 'ntal', 'ntol', 'hydl'],\n",
    "    'ngl': ['ib', 'tugo', 'ecco2',  'merra2', 'ntal', 'ntol', 'hydl'], \n",
    "    'ay': ['ib', 'tugo', 'ecco2',  'merra2', 'ntal', 'ntol', 'hydl']\n",
    "}\n",
    "colnames = {\n",
    "    'gr': ['east', 'north', 'up'],\n",
    "    'tud': ['east', 'north', 'up'],\n",
    "    'osu': ['east', 'north', 'up'],\n",
    "    'ngl': ['east', 'north', 'up'],\n",
    "    'ay': ['east', 'north', 'up']\n",
    "}\n",
    "\n",
    "# Example input structure: aboa_gr, aboa_tud, aboa_ntal, etc.\n",
    "stations = ['aboa', 'syog', 'vesl']\n",
    "all_datasets = {\n",
    "    'aboa': {\n",
    "        'gr': aboa_gr, 'tud': aboa_tud, 'osu': aboa_osu, 'ay': aboa_ay,\n",
    "        'ib': aboa_era5ib, 'tugo': aboa_era5tugo, 'ecco2': aboa_ecco2,\n",
    "        'merra2': aboa_merra2, 'ntal': aboa_ntal, 'ntol': aboa_ntol,\n",
    "        'hydl': aboa_hydl  \n",
    "    },\n",
    "    'syog': {\n",
    "        'gr': syog_gr, 'tud': syog_tud, 'osu': syog_osu, 'ngl': syog_ngl, 'ay': syog_ay,\n",
    "        'ib': syog_era5ib, 'tugo': syog_era5tugo, 'ecco2': syog_ecco2,\n",
    "        'merra2': syog_merra2, 'ntal': syog_ntal, 'ntol': syog_ntol,\n",
    "        'hydl': syog_hydl\n",
    "    },\n",
    "    'vesl': {\n",
    "        'gr': vesl_gr, 'tud': vesl_tud, 'osu': vesl_osu, 'ngl': vesl_ngl, 'ay':vesl_ay,\n",
    "        'ib': vesl_era5ib, 'tugo': vesl_era5tugo, 'ecco2': vesl_ecco2,\n",
    "        'merra2': vesl_merra2, 'ntal': vesl_ntal, 'ntol': vesl_ntol,\n",
    "        'hydl': vesl_hydl\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3feb5a6d-45c7-4211-9429-38a0b4730c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aboa_ngl not available — skipping.\n"
     ]
    }
   ],
   "source": [
    "# Final results\n",
    "merged_all = {}\n",
    "\n",
    "for base_key in base_keys:\n",
    "    merged_results = {}\n",
    "\n",
    "    for station in stations:\n",
    "        dfs = all_datasets.get(station, {})\n",
    "\n",
    "        if base_key not in dfs:\n",
    "            print(f\"{station}_{base_key} not available — skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df_base = dfs[base_key].copy()\n",
    "            df_base['date'] = pd.to_datetime(df_base['date'])\n",
    "            base_cols = colnames[base_key]\n",
    "            cols_to_use = ['date', 'MJD'] + base_cols if 'MJD' in df_base.columns else ['date'] + base_cols\n",
    "            merged = df_base[cols_to_use].copy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {station}_{base_key}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for suffix in merge_suffix[base_key]:\n",
    "            if suffix not in dfs:\n",
    "                print(f\"{station}_{suffix} not available — skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df = dfs[suffix].copy()\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                merge_cols = [f'east_{suffix}', f'north_{suffix}', f'up_{suffix}']\n",
    "                available_cols = [col for col in merge_cols if col in df.columns]\n",
    "                merged = pd.merge(\n",
    "                    merged,\n",
    "                    df[['date'] + available_cols],\n",
    "                    on='date',\n",
    "                    how='left'\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error merging {station}_{suffix} into {station}_{base_key}: {e}\")\n",
    "                continue\n",
    "\n",
    "        merged_results[station] = merged\n",
    "\n",
    "    merged_all[base_key] = merged_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0dfac53-3231-4d80-9798-a819e96627f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset-specific correction suffixes\n",
    "gr_only_corrections = {'_c_ecco2', '_c_ntol', '_c_all_b'}\n",
    "other_datasets_corrections = {\n",
    "    '_c_ib', '_c_tugo', '_c_merra2', '_c_ntal',\n",
    "    '_c_ibecco2', '_c_merra2ecco2', '_c_ntalntol', '_c_all'\n",
    "}\n",
    "\n",
    "# Define all correction rules\n",
    "corrections = [\n",
    "    ('_c_ecco2', ['east', 'north', 'up'], ['east_ecco2', 'north_ecco2', 'up_ecco2']),\n",
    "    ('_c_ntol', ['east', 'north', 'up'], ['east_ntol', 'north_ntol', 'up_ntol']),\n",
    "    ('_c_all_b', ['east', 'north', 'up'], ['east_ntol', 'north_ntol', 'up_ntol', 'east_hydl', 'north_hydl', 'up_hydl']),\n",
    "\n",
    "    ('_c_ib', ['east', 'north', 'up'], ['east_ib', 'north_ib', 'up_ib']),\n",
    "    ('_c_tugo', ['east', 'north', 'up'], ['east_tugo', 'north_tugo', 'up_tugo']),\n",
    "    ('_c_merra2', ['east', 'north', 'up'], ['east_merra2', 'north_merra2', 'up_merra2']),\n",
    "    ('_c_ntal', ['east', 'north', 'up'], ['east_ntal', 'north_ntal', 'up_ntal']),\n",
    "    ('_c_ibecco2', ['east', 'north', 'up'], ['east_ib', 'east_ecco2', 'north_ib', 'north_ecco2', 'up_ib', 'up_ecco2']), \n",
    "    ('_c_merra2ecco2', ['east', 'north', 'up'], ['east_merra2', 'east_ecco2', 'north_merra2', 'north_ecco2', 'up_merra2', 'up_ecco2']),\n",
    "    ('_c_ntalntol', ['east', 'north', 'up'], ['east_ntal', 'east_ntol', 'north_ntal', 'north_ntol', 'up_ntal', 'up_ntol']),\n",
    "    ('_c_all', ['east', 'north', 'up'], \n",
    "     ['east_ntal', 'east_ntol', 'east_hydl','north_ntal', 'north_ntol', 'north_hydl','up_ntal', 'up_ntol', 'up_hydl']),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "111f41ee-a61f-439a-9058-adaadd008175",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_key, station_dict in merged_all.items():  # e.g., 'gr', 'ngl'\n",
    "    if dataset_key == 'gr':\n",
    "        valid_suffixes = gr_only_corrections\n",
    "    else:\n",
    "        valid_suffixes = other_datasets_corrections\n",
    "\n",
    "    for station, df in station_dict.items():  # e.g., 'aboa', 'syog'\n",
    "        if df is None or not isinstance(df, pd.DataFrame):\n",
    "            print(f\"{dataset_key}_{station} is not a valid DataFrame — skipping\")\n",
    "            continue\n",
    "\n",
    "        for suffix, coords, corr_cols in corrections:\n",
    "            if suffix not in valid_suffixes:\n",
    "                continue\n",
    "\n",
    "            for i, coord in enumerate(coords):\n",
    "                col_name = f\"{coord}{suffix}\"\n",
    "                base_col = coord\n",
    "\n",
    "                # One-term correction\n",
    "                if len(corr_cols) == 3:\n",
    "                    term_col = corr_cols[i]\n",
    "                    if term_col in df.columns:\n",
    "                        df[col_name] = df[base_col] - df[term_col]\n",
    "                    else:\n",
    "                        print(f\"{col_name} skipped: missing {term_col} in {dataset_key}_{station}\")\n",
    "                else:\n",
    "                    # Multi-term correction\n",
    "                    terms = [t for t in corr_cols if t.startswith(coord)]\n",
    "                    if all(t in df.columns for t in terms):\n",
    "                        df[col_name] = df[base_col] - sum(df[t] for t in terms)\n",
    "                    else:\n",
    "                        print(f\"{col_name} skipped: missing terms for {coord} in {dataset_key}_{station}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f25d2e1d-6ec7-4492-b2a3-9f0df6dfacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_key, station_dict in merged_all.items():\n",
    "    for station, df in station_dict.items():\n",
    "        if df is not None and isinstance(df, pd.DataFrame):\n",
    "            df.to_csv(f\"GNSS/corrected/{station}_{dataset_key}_corrections.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307a944-3bc4-4cca-82df-98162a09fcb4",
   "metadata": {},
   "source": [
    "# Save data\n",
    "For Hector analysis, the GNSS time series are converted into `.mom` files. Each file corresponds to a single station, dataset, correction, and displacement component. Filenames follow the convention `XXYY_Z.mom`, where `XX` encodes the station and dataset, `YY` identifies the correction (with `00` for uncorrected), and `Z` indicates the component (`0` = east, `1` = north, `2` = up). Each file begins with a comment line (`# sampling period 1`) followed by the MJD and displacement data without headers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1dfe41a-91c2-460a-8c9e-0e836bd2a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write a .mom file with a comment at the first row\n",
    "def write_mom_with_comment(df, filename):\n",
    "    comment = 'sampling period 1'\n",
    "\n",
    "    # Determine fractional part from first timestamp in data\n",
    "    first_mjd = df.iloc[0, 0]  # First value in MJD column\n",
    "    frac_part = first_mjd - int(first_mjd)\n",
    "\n",
    "    # Default steps\n",
    "    steps = []\n",
    "    if filename[21] == 'A':\n",
    "        steps = [\n",
    "            \"2003-02-01\", \"2007-12-31\", \"2008-01-03\", \"2009-02-04\", \"2009-02-05\",\n",
    "            \"2010-01-12\", \"2010-02-05\", \"2011-01-11\", \"2011-01-13\", \"2012-01-15\",\n",
    "            \"2012-01-17\", \"2013-01-09\", \"2013-01-10\", \"2014-01-23\", \"2014-01-24\",\n",
    "            \"2015-01-05\", \"2015-01-06\", \"2016-01-05\", \"2016-01-06\", \"2017-02-02\",\n",
    "            \"2017-02-06\"\n",
    "        ]\n",
    "\n",
    "    elif filename[21] == 'S':\n",
    "        steps = [\"2007-01-26\", \"2013-12-23\", \"2020-01-18\"]\n",
    "\n",
    "    elif filename[21] == 'V':\n",
    "        steps = [\"2012-01-08\", \"2012-08-21\"]\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"# {comment}\\n\")\n",
    "\n",
    "        # Write Hector offsets with same fractional part as first observation\n",
    "        for step in steps:\n",
    "            mjd = datetime_to_mjd(step) + frac_part\n",
    "            f.write(f\"# offset {mjd:.6f}\\n\")\n",
    "\n",
    "    # Append the data without headers\n",
    "    df.to_csv(filename, sep=' ', index=False, header=False, float_format='%.6f', mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a98fc3b8-9262-4a9a-88be-01dd1cbe45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map station and dataset to 1-letter or 2-letter codes\n",
    "station_map = {\n",
    "    'aboa': 'A',\n",
    "    'syog': 'S',\n",
    "    'vesl': 'V'\n",
    "}\n",
    "dataset_map = {\n",
    "    'gr': 'G',\n",
    "    'ngl': 'N',\n",
    "    'tud': 'T',\n",
    "    'osu': 'O',\n",
    "    'ay': 'A'\n",
    "}\n",
    "correction_map = {\n",
    "    'ecco2': '01',\n",
    "    'ntol': '02',\n",
    "    'all_b': '03',\n",
    "    'ib': '04',\n",
    "    'tugo': '05',\n",
    "    'merra2': '06',\n",
    "    'ntal': '07',\n",
    "    'ibecco2': '08',\n",
    "    'merra2ecco2': '09',\n",
    "    'ntalntol': '10',\n",
    "    'all': '11'\n",
    "}\n",
    "\n",
    "# Components and output\n",
    "components = ['east', 'north', 'up']\n",
    "output_dir = 'hector/mom_files_all'\n",
    "\n",
    "# Loop through nested merged_all structure\n",
    "for dataset, station_dict in merged_all.items():\n",
    "    for station, df in station_dict.items():\n",
    "        if df is None or not isinstance(df, pd.DataFrame):\n",
    "            continue\n",
    "\n",
    "        st_code = station_map.get(station, station[:2].upper())\n",
    "        ds_code = dataset_map.get(dataset, dataset[:2].upper())\n",
    "        base_code = st_code + ds_code \n",
    "\n",
    "        # Save original components\n",
    "        for i, comp in enumerate(components):\n",
    "            if comp in df.columns:\n",
    "                filename = os.path.join(output_dir, f\"{base_code}00_{i}.mom\")\n",
    "                write_mom_with_comment(df[['MJD', comp]], filename)\n",
    "\n",
    "        # Save corrected components\n",
    "        for col in df.columns:\n",
    "            if any(col.startswith(f\"{comp}_c_\") for comp in components):\n",
    "                comp_type, _, corr = col.partition('_c_')\n",
    "                idx = components.index(comp_type)\n",
    "                corr_code = correction_map.get(corr, corr[:2].upper())\n",
    "                filename = os.path.join(output_dir, f\"{base_code}{corr_code}_{idx}.mom\")\n",
    "                write_mom_with_comment(df[['MJD', col]], filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
